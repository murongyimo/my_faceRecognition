{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import inference\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 处理好之后的数据文件。\n",
    "INPUT_DATA = './dataset/orl_faces.npy'\n",
    "# 保存训练好的模型的路径。\n",
    "MODEL_SAVE_PATH = './model/'\n",
    "MODEL_NAME = 'face_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(images, labels, num_examples, batch_size, index_in_epoch):\n",
    "    x = []\n",
    "    y = []\n",
    "    next_index = index_in_epoch + batch_size\n",
    "    if next_index > num_examples:\n",
    "        next_index = next_index % num_examples\n",
    "        x1 = images[index_in_epoch:]\n",
    "        x2 = images[:next_index]\n",
    "        y1 = labels[index_in_epoch:]\n",
    "        y2 = labels[:next_index]\n",
    "        x_ = np.vstack((x1, x2))\n",
    "        y_ = np.vstack((y1, y2))\n",
    "    elif next_index < num_examples:\n",
    "        x.append(images[index_in_epoch:next_index])\n",
    "        y.append(labels[index_in_epoch:next_index])\n",
    "        x_ = np.asarray(x)\n",
    "        y_ = np.asarray(y)\n",
    "    else:\n",
    "        next_index = next_index % num_examples\n",
    "        x.append(images[index_in_epoch:])\n",
    "        y.append(labels[index_in_epoch:])\n",
    "        x_ = np.asarray(x)\n",
    "        y_ = np.asarray(y)\n",
    "    index_in_epoch = next_index\n",
    "    \n",
    "    return (x_, y_, index_in_epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "#     print(type(training_data[0]))\n",
    "    index_in_epoch = 0\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            inference.IMAGE_SIZE,\n",
    "            inference.IMAGE_SIZE,\n",
    "            inference.NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, inference.OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference.inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    print(\"11111111111\")\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        len(training_data[0]) / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    print('222222222222222')\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    print('3333333333333333')\n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "#         print('initlized')\n",
    "        print('444444444444444')\n",
    "    \n",
    "    \n",
    "#----------------------------------------    \n",
    "    \n",
    "#         for i in range(TRAINING_STEPS):\n",
    "#             print('5555555555-------',i)\n",
    "# #             xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "#             xs, ys, index_in_epoch= next_batch(training_data[0],training_data[1],len(training_data[0]), BATCH_SIZE, index_in_epoch)\n",
    "#             reshaped_xs = np.reshape(xs, (\n",
    "#                 BATCH_SIZE,\n",
    "#                 inference.IMAGE_SIZE,\n",
    "#                 inference.IMAGE_SIZE,\n",
    "#                 inference.NUM_CHANNELS))\n",
    "#             _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "#----------------------------------------------------------            \n",
    "        start = 0\n",
    "        end = BATCH_SIZE\n",
    "        n_training_example = len(training_data[0])\n",
    "        for i in range(TRAINING_STEPS):\n",
    "#             xs = training_data[0][start:end]\n",
    "#             ys = training_data[1][start:end]\n",
    "#             start = end\n",
    "#             if start == n_training_example:\n",
    "#                 start = 0\n",
    "#             end = start + BATCH_SIZE\n",
    "#             if end > n_training_example: \n",
    "#                 end = end - n_training_example\n",
    "#                 xs = np.vstack((xs, training_data[0][:end]))\n",
    "#                 ys = np.vstack((ys, training_data[1][:end]))\n",
    "            if start > end:\n",
    "                x1 = training_data[0][start:]\n",
    "                x2 = training_data[0][:end]\n",
    "                y1 = training_data[1][start:]\n",
    "                y2 = training_data[1][:end]\n",
    "                xs = np.vstack((x1, x2))\n",
    "                ys = np.vstack((y1, y2))\n",
    "            else:\n",
    "                xs = training_data[0][start:end]\n",
    "                ys = training_data[1][start:end]\n",
    "            start = end\n",
    "            end = end + BATCH_SIZE\n",
    "            if start == n_training_example:\n",
    "                start = 0\n",
    "            if end > n_training_example:\n",
    "                end = end - n_training_example\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                inference.IMAGE_SIZE,\n",
    "                inference.IMAGE_SIZE,\n",
    "                inference.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "#--------------------------------------------------\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step=global_step)\n",
    "#             print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 training examples has loaded.\n",
      "<class 'numpy.ndarray'>\n",
      "11111111111\n",
      "222222222222222\n",
      "3333333333333333\n",
      "444444444444444\n",
      "After 1 training step(s), loss on training batch is 25.469.\n",
      "After 1001 training step(s), loss on training batch is 9.93834.\n",
      "After 2001 training step(s), loss on training batch is 9.93369.\n",
      "After 3001 training step(s), loss on training batch is 9.92919.\n",
      "After 4001 training step(s), loss on training batch is 9.92861.\n",
      "After 5001 training step(s), loss on training batch is 9.92841.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    photo_data = np.load(INPUT_DATA)\n",
    "    training_images = np.asarray(photo_data[0])\n",
    "    training_labels = np.asarray(photo_data[1])\n",
    "    n_training_example = len(training_images)\n",
    "    print(\"%d training examples has loaded.\" % (n_training_example))\n",
    "    print(type(training_images))\n",
    "    train([training_images, training_labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
